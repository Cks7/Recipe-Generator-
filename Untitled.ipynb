{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b34d3e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0                  title  \\\n",
      "0           0    No-Bake Nut Cookies   \n",
      "1           1  Jewell Ball'S Chicken   \n",
      "2           2            Creamy Corn   \n",
      "3           3          Chicken Funny   \n",
      "4           4   Reeses Cups(Candy)     \n",
      "\n",
      "                                         ingredients  \\\n",
      "0  [\"1 c. firmly packed brown sugar\", \"1/2 c. eva...   \n",
      "1  [\"1 small jar chipped beef, cut up\", \"4 boned ...   \n",
      "2  [\"2 (16 oz.) pkg. frozen corn\", \"1 (8 oz.) pkg...   \n",
      "3  [\"1 large whole chicken\", \"2 (10 1/2 oz.) cans...   \n",
      "4  [\"1 c. peanut butter\", \"3/4 c. graham cracker ...   \n",
      "\n",
      "                                          directions  \\\n",
      "0  [\"In a heavy 2-quart saucepan, mix brown sugar...   \n",
      "1  [\"Place chipped beef on bottom of baking dish....   \n",
      "2  [\"In a slow cooker, combine all ingredients. C...   \n",
      "3  [\"Boil and debone chicken.\", \"Put bite size pi...   \n",
      "4  [\"Combine first four ingredients and press in ...   \n",
      "\n",
      "                                              link    source  \\\n",
      "0   www.cookbooks.com/Recipe-Details.aspx?id=44874  Gathered   \n",
      "1  www.cookbooks.com/Recipe-Details.aspx?id=699419  Gathered   \n",
      "2   www.cookbooks.com/Recipe-Details.aspx?id=10570  Gathered   \n",
      "3  www.cookbooks.com/Recipe-Details.aspx?id=897570  Gathered   \n",
      "4  www.cookbooks.com/Recipe-Details.aspx?id=659239  Gathered   \n",
      "\n",
      "                                                 NER  \n",
      "0  [\"brown sugar\", \"milk\", \"vanilla\", \"nuts\", \"bu...  \n",
      "1  [\"beef\", \"chicken breasts\", \"cream of mushroom...  \n",
      "2  [\"frozen corn\", \"cream cheese\", \"butter\", \"gar...  \n",
      "3  [\"chicken\", \"chicken gravy\", \"cream of mushroo...  \n",
      "4  [\"peanut butter\", \"graham cracker crumbs\", \"bu...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('RecipeNLG_dataset.csv')\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "982739bf",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 22.4 GiB for an array with shape (2231142, 2697) and data type int32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m max_seq_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m([\u001b[38;5;28mlen\u001b[39m(seq) \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m ingredient_sequences \u001b[38;5;241m+\u001b[39m direction_sequences])\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Pad sequences to ensure uniform length\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m ingredient_sequences \u001b[38;5;241m=\u001b[39m \u001b[43mpad_sequences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mingredient_sequences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxlen\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_seq_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m direction_sequences \u001b[38;5;241m=\u001b[39m pad_sequences(direction_sequences, maxlen\u001b[38;5;241m=\u001b[39mmax_seq_len, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\keras\\src\\utils\\sequence_utils.py:113\u001b[0m, in \u001b[0;36mpad_sequences\u001b[1;34m(sequences, maxlen, dtype, padding, truncating, value)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m dtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dtype_str:\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    108\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`dtype` \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not compatible with `value`\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms type: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    109\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mYou should set `dtype=object` for variable length \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    110\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrings.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    111\u001b[0m     )\n\u001b[1;32m--> 113\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfull\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxlen\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msample_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(sequences):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\numpy\\core\\numeric.py:329\u001b[0m, in \u001b[0;36mfull\u001b[1;34m(shape, fill_value, dtype, order, like)\u001b[0m\n\u001b[0;32m    327\u001b[0m     fill_value \u001b[38;5;241m=\u001b[39m asarray(fill_value)\n\u001b[0;32m    328\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m fill_value\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m--> 329\u001b[0m a \u001b[38;5;241m=\u001b[39m \u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    330\u001b[0m multiarray\u001b[38;5;241m.\u001b[39mcopyto(a, fill_value, casting\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124munsafe\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m a\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 22.4 GiB for an array with shape (2231142, 2697) and data type int32"
     ]
    }
   ],
   "source": [
    "# Combine ingredients and directions for tokenization\n",
    "combined_text = df['ingredients'].astype(str) + \" \" + df['directions'].astype(str)\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(combined_text)\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Convert text to sequences\n",
    "ingredient_sequences = tokenizer.texts_to_sequences(df['ingredients'])\n",
    "direction_sequences = tokenizer.texts_to_sequences(df['directions'])\n",
    "\n",
    "# Find the maximum sequence length\n",
    "max_seq_len = max([len(seq) for seq in ingredient_sequences + direction_sequences])\n",
    "\n",
    "# Pad sequences to ensure uniform length\n",
    "ingredient_sequences = pad_sequences(ingredient_sequences, maxlen=max_seq_len, padding='post')\n",
    "direction_sequences = pad_sequences(direction_sequences, maxlen=max_seq_len, padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2a551c1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_test_split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Splitting data into training and testing sets\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m(ingredient_sequences, direction_sequences, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# One-hot encode the target sequences\u001b[39;00m\n\u001b[0;32m      5\u001b[0m y_train_one_hot \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mto_categorical(y_train, num_classes\u001b[38;5;241m=\u001b[39mtotal_words)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_test_split' is not defined"
     ]
    }
   ],
   "source": [
    "# Splitting data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(ingredient_sequences, direction_sequences, test_size=0.2, random_state=42)\n",
    "\n",
    "# One-hot encode the target sequences\n",
    "y_train_one_hot = tf.keras.utils.to_categorical(y_train, num_classes=total_words)\n",
    "y_test_one_hot = tf.keras.utils.to_categorical(y_test, num_classes=total_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58e16b11",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Sequential' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSequential\u001b[49m()\n\u001b[0;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Embedding(input_dim\u001b[38;5;241m=\u001b[39mtotal_words, output_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, input_length\u001b[38;5;241m=\u001b[39mmax_seq_len))\n\u001b[0;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39madd(LSTM(\u001b[38;5;241m150\u001b[39m, return_sequences\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Sequential' is not defined"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=total_words, output_dim=100, input_length=max_seq_len))\n",
    "model.add(LSTM(150, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(150))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8593965b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mfit(X_train, y_train_one_hot, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, validation_data\u001b[38;5;241m=\u001b[39m(X_test, y_test_one_hot))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train_one_hot, epochs=20, batch_size=64, validation_data=(X_test, y_test_one_hot))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64f536cd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m loss, accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mevaluate(X_test, y_test_one_hot)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test_one_hot)\n",
    "print(f\"Test Loss: {loss}\")\n",
    "print(f\"Test Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "862c7b44",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m     14\u001b[0m seed_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2 cups flour 1 cup sugar 1/2 cup butter\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 15\u001b[0m generated_recipe \u001b[38;5;241m=\u001b[39m generate_recipe(\u001b[43mmodel\u001b[49m, tokenizer, seed_text, max_seq_len)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(generated_recipe)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "def generate_recipe(model, tokenizer, seed_text, max_seq_len):\n",
    "    for _ in range(100):  # Generate up to 100 words\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_seq_len-1, padding='pre')\n",
    "        predicted = model.predict(token_list, verbose=0)\n",
    "        predicted_word_index = np.argmax(predicted, axis=1)\n",
    "        predicted_word = tokenizer.index_word[predicted_word_index[0]]\n",
    "        seed_text += \" \" + predicted_word\n",
    "        if predicted_word == 'end':  # Assuming 'end' token marks the end of the recipe\n",
    "            break\n",
    "    return seed_text\n",
    "\n",
    "# Example usage\n",
    "seed_text = \"2 cups flour 1 cup sugar 1/2 cup butter\"\n",
    "generated_recipe = generate_recipe(model, tokenizer, seed_text, max_seq_len)\n",
    "print(generated_recipe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c394b27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
